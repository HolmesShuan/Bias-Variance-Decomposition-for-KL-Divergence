# Bias-Variance-Decomposition-for-KL-Divergence
This repository includes some detailed proofs of "Bias Variance Decomposition for KL Divergence". Hopefully, it will be helpful for a better understanding of [Heskes's paper](https://direct.mit.edu/neco/article-pdf/10/6/1425/813893/089976698300017232.pdf) and recent ICLR paper ["Rethinking Soft Labels for Knowledge Distillation: A Bias-Variance Tradeoff Perspective"](https://openreview.net/pdf?id=gIHd-5X324). 

## Proofs:

## Reference:
1. [Briefly answered in math.stackexchange.com](https://math.stackexchange.com/questions/3017916/bias-variance-decomposition-for-kl-divergence)
2. [Paper: Information-Theoretic Variable Selection and Network Inference from Microarray Data](https://dipot.ulb.ac.be/dspace/bitstream/2013/210396/1/baf3a39e-3c11-496d-8b3b-b952a1827ca0.txt)
4. [Paper: Bias/variance decompositions for likelihood-based estimators](https://direct.mit.edu/neco/article-pdf/10/6/1425/813893/089976698300017232.pdf)
5. [Book: Notes for EE2211 Introduction to Machine Learning](https://vyftan.github.io/papers/ee2211book.pdf)
